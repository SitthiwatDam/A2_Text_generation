{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP:Assignment 2 Text Generation\n",
    "Name: Sitthiwat Damrongpreechar <br>\n",
    "Student ID: st123994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import libraies and setting up the device\n",
    "Before getting started, I need to import all the necessary libraries, especially PyTorch, which is mainly used for performing LSTM-LM. Additionally, tqdm is imported to handle the loading stage, aiding in checking the training progress and estimating time. Since my laptop has CUDA support, I will set the device to 'cuda' for better performance in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torchtext, datasets, math    \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0+cu121', '0.17.0+cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the versions of torch and torchtext\n",
    "torch.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting up the device\n",
    "device=  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Harry Potter Theme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the text generator, a specific theme needs to be chosen. In this case, I have selected the Harry Potter theme. The dataset used for this project is sourced from the [Hugging Faces](https://huggingface.co/) dataset named [\"novel-text\", created by OswaldHe123.](https://huggingface.co/datasets/OswaldHe123/novel-text) This dataset is derived from the book [\"HARRY POTTER AND THE GOBLET OF FIRE.\"](https://en.wikipedia.org/wiki/Harry_Potter_and_the_Goblet_of_Fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 54142\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 5996\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 9116\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from huggingface datasets\n",
    "datasets = datasets.load_dataset(\"OswaldHe123/novel-text\")\n",
    "# print the datasets\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above output, the datasets contain 3 datasets; train, test, and validation. The number of rows in each dataset is as follows: 54142, 9116, and 5996 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the preprocessing, Tokenizing and Numericalizing technique would be used to prepare the tokenized datasets and vocabualry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# create the function to tokenize the data\n",
    "tokenize_data = lambda example, tokenizer:{'tokens':tokenizer(example['text'])}\n",
    "# tokenize the data and remove the text column\n",
    "tokenized_datasets = datasets.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer':tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', ',', 'all', 'right', 'then', '.', 'you', 'can', 'go', 'to', 'this', 'ruddy', '.', '.', '.', 'this', 'stupid', '.', '.', '.', 'this', 'world', 'cup', 'thing', '.', 'you', 'write', 'and', 'tell', 'these', '-', 'these', 'weasleys', 'they', \"'\", 're', 'to', 'pick', 'you', 'up', ',', 'mind', '.', 'i', 'haven', \"'\", 't', 'got', 'time', 'to', 'go', 'dropping', 'you', 'off', 'all', 'over', 'the', 'country', '.', 'and', 'you', 'can', 'spend', 'the', 'rest', 'of', 'the', 'summer', 'there', '.', 'and', 'you', 'can', 'tell', 'your', '-', 'your', 'godfather', '.', '.', '.', 'tell', 'him', '.', '.', '.', 'tell', 'him', 'you', \"'\", 're', 'going', '.']\n"
     ]
    }
   ],
   "source": [
    "# check the tokenized data\n",
    "print(tokenized_datasets['train'][224]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Numericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_datasets['train']['tokens'], min_freq=3)\n",
    "# insert the special tokens\n",
    "vocab.insert_token('<unk>',0)\n",
    "vocab.insert_token('<eos>',1)\n",
    "# set the default index\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15174\n"
     ]
    }
   ],
   "source": [
    "# check the vocab size\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', ',', '.', 'the', \"'\", 'and', 'to', 'of', 'a']\n"
     ]
    }
   ],
   "source": [
    "# check the vocab tokens of the first 10 indices\n",
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the function to prepare the data for training\n",
    "def get_data(datasets, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in datasets:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size #from example 12 // 3 = 4 #integer division\n",
    "    data = data[:num_batches*batch_size]\n",
    "    data = data.view(batch_size, num_batches) # 3,4 #view vs reshape (whether data is contiguous or not)\n",
    "    return data # [batch_size, seq len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the batch size and prepare the train, test, validation data\n",
    "batch_size = 128\n",
    "train_data = get_data(tokenized_datasets['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_datasets['validation'], vocab, batch_size)\n",
    "test_data = get_data(tokenized_datasets['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 13086])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the train data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's Architechture\n",
    "The LSTM model is constructed as a class structure, comprising layers for embedding to convert input tokens into continuous vectors, LSTM processing, dropout for regularization, and fully connected for output. The model is designed to predict the next token in a sequence. Additional attributes include initializing weights and handling hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the class of LSTM language model\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    # define the class attributes\n",
    "    def __init__(self,vocab_size, hid_dim, emb_dim, num_layers,dropout_rate):\n",
    "        super().__init__()\n",
    "        # define the number of layers\n",
    "        self.num_layers = num_layers\n",
    "        # define the hidden layer\n",
    "        self.hid_dim = hid_dim\n",
    "        # define the embedding layer\n",
    "        self.emb_dim = emb_dim\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        # define the LSTM layer\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, \n",
    "                            num_layers=num_layers, dropout=dropout_rate, \n",
    "                            batch_first=True) #dropout is applied to the output of each LSTM layer except the last layer\n",
    "        # define the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # define the fully connected layer\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        # define the initialize the weights function\n",
    "        self.init_weights()\n",
    "    # define the function to initialize the weights\n",
    "    def init_weights(self):\n",
    "        # define the parameters for the embedding layer\n",
    "        init_range_emb = 0.1\n",
    "        # define the parameters for the other layers\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        # make the embedding layer weights uniform\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        # make the fully connected layer weights uniform\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        # make the fully connected layer bias zero\n",
    "        self.fc.bias.data.zero_()\n",
    "        # make the LSTM layer weights uniform\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    # define the function to initialize the hidden layer's weights\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    # define the function to detach the hidden layer\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden  = hidden.detach() # not to be used for gradient calculation\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "    # define the forward pass\n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq_len]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src)) \n",
    "        # embedding: [batch_size, seq_len, emb_dim]\n",
    "\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # output: [batch_size, seq_len, hid_dim]\n",
    "        # hidden: [num_layers * direction, seq_len, hid_dim]\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        # prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for the model\n",
    "vocab_size = len(vocab)\n",
    "emb_dim = 400 # embedding dimension (in video used 1024, 400 in paper)\n",
    "hid_dim = 800 # hidden dimension (in video used 1024, 1150 in paper)\n",
    "num_layers = 2 # number of layers (in video used 2, 3 in paper)\n",
    "dropout_rate = 0.65\n",
    "# define the learning rate\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 27,196,774 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# assign the model to the device\n",
    "model = LSTMLanguageModel(vocab_size, hid_dim, emb_dim, num_layers, dropout_rate).to(device)\n",
    "# define the optimizer that is Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# define the loss function that is cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# define the variable to check the number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batch function for training \n",
    "def get_batch(data, seq_len, idx):\n",
    "    #data # [batch_size, bunch of tokens]\n",
    "    src = data[:,idx:idx+seq_len]\n",
    "    target = data[:,idx+1:idx+1+seq_len] # shifted ahead src by 1\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to train the model\n",
    "def train(model, data, optimizer, criterion,batch_size,seq_len, clip, device):\n",
    "    # define the epoch loss\n",
    "    epoch_loss = 0\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    #drop all batches that are not a multiple of seq_len\n",
    "    # data # [batch_size, bunch of tokens(seq_len)]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    #loop through the batches\n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        #zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        #get the batch\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        #backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        #update\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to evaluate the model\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    # define the epoch loss\n",
    "    epoch_loss = 0\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    #loop through the batches without updating the gradients\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Perplexity: 802.668\n",
      "\tValid Perplexity: 466.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Perplexity: 445.931\n",
      "\tValid Perplexity: 331.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tTrain Perplexity: 336.345\n",
      "\tValid Perplexity: 255.292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04\n",
      "\tTrain Perplexity: 275.423\n",
      "\tValid Perplexity: 216.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05\n",
      "\tTrain Perplexity: 239.501\n",
      "\tValid Perplexity: 189.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06\n",
      "\tTrain Perplexity: 210.942\n",
      "\tValid Perplexity: 166.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07\n",
      "\tTrain Perplexity: 189.408\n",
      "\tValid Perplexity: 151.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08\n",
      "\tTrain Perplexity: 173.907\n",
      "\tValid Perplexity: 144.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09\n",
      "\tTrain Perplexity: 161.899\n",
      "\tValid Perplexity: 133.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "\tTrain Perplexity: 152.305\n",
      "\tValid Perplexity: 125.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "\tTrain Perplexity: 144.590\n",
      "\tValid Perplexity: 121.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "\tTrain Perplexity: 137.964\n",
      "\tValid Perplexity: 115.160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "\tTrain Perplexity: 132.526\n",
      "\tValid Perplexity: 111.496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "\tTrain Perplexity: 127.688\n",
      "\tValid Perplexity: 109.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "\tTrain Perplexity: 123.206\n",
      "\tValid Perplexity: 105.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n",
      "\tTrain Perplexity: 119.462\n",
      "\tValid Perplexity: 102.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n",
      "\tTrain Perplexity: 115.941\n",
      "\tValid Perplexity: 100.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n",
      "\tTrain Perplexity: 112.651\n",
      "\tValid Perplexity: 98.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n",
      "\tTrain Perplexity: 109.866\n",
      "\tValid Perplexity: 96.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\n",
      "\tTrain Perplexity: 107.068\n",
      "\tValid Perplexity: 94.168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\n",
      "\tTrain Perplexity: 104.672\n",
      "\tValid Perplexity: 92.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22\n",
      "\tTrain Perplexity: 102.405\n",
      "\tValid Perplexity: 91.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23\n",
      "\tTrain Perplexity: 100.081\n",
      "\tValid Perplexity: 89.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24\n",
      "\tTrain Perplexity: 98.232\n",
      "\tValid Perplexity: 88.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25\n",
      "\tTrain Perplexity: 96.519\n",
      "\tValid Perplexity: 87.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26\n",
      "\tTrain Perplexity: 94.549\n",
      "\tValid Perplexity: 85.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27\n",
      "\tTrain Perplexity: 93.324\n",
      "\tValid Perplexity: 85.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28\n",
      "\tTrain Perplexity: 91.711\n",
      "\tValid Perplexity: 84.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29\n",
      "\tTrain Perplexity: 89.859\n",
      "\tValid Perplexity: 83.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n",
      "\tTrain Perplexity: 88.593\n",
      "\tValid Perplexity: 82.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31\n",
      "\tTrain Perplexity: 87.303\n",
      "\tValid Perplexity: 81.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32\n",
      "\tTrain Perplexity: 86.165\n",
      "\tValid Perplexity: 80.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33\n",
      "\tTrain Perplexity: 84.732\n",
      "\tValid Perplexity: 80.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34\n",
      "\tTrain Perplexity: 83.627\n",
      "\tValid Perplexity: 79.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35\n",
      "\tTrain Perplexity: 82.483\n",
      "\tValid Perplexity: 78.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36\n",
      "\tTrain Perplexity: 81.447\n",
      "\tValid Perplexity: 78.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37\n",
      "\tTrain Perplexity: 80.452\n",
      "\tValid Perplexity: 77.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38\n",
      "\tTrain Perplexity: 79.470\n",
      "\tValid Perplexity: 76.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39\n",
      "\tTrain Perplexity: 78.885\n",
      "\tValid Perplexity: 76.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40\n",
      "\tTrain Perplexity: 77.778\n",
      "\tValid Perplexity: 76.214\n"
     ]
    }
   ],
   "source": [
    "# define the parameters for training\n",
    "n_epochs = 40\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "# define the learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "# define the best validation loss\n",
    "best_valid_loss = float('inf')\n",
    "# loop through the epochs\n",
    "for epoch in range(n_epochs):\n",
    "    # call the train and evaluate functions to get the losses\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "    # update the learning rate scheduler\n",
    "    lr_scheduler.step(valid_loss)\n",
    "    # save the model state dict if the validation loss is the best\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "    # print the epoch, train perplexity and validation perplexity\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is finished, I need to check the model's perplexity. The trained model will be loaded to test on the test data, and then I will proceed to evaluate and calculate the test perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 114.026\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model for testing\n",
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "# Evaluate the model on the test data and calculate test perplexity\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the inference step, the model will be later utilized in an application. Therefore, I need to create the function to generate the output following these steps:\n",
    "1. **Input Processing:**\n",
    "   - Take a given prompt and tokenize it, preparing it for model input.\n",
    "\n",
    "2. **Model Prediction:**\n",
    "   - Encode the tokenized prompt and feed it into the model to obtain predictions.\n",
    "   - Apply softmax to the model's output, focusing on the prediction for the next word.\n",
    "   - Adjust model confidence by dividing logits with a specified temperature value.\n",
    "\n",
    "3. **Random Sampling:**\n",
    "   - Randomly sample from the softmax distribution to predict the next word.\n",
    "   - Retry if an unknown token (`<unk>`) is encountered during sampling.\n",
    "\n",
    "4. **Completion Criteria:**\n",
    "   - Continue predicting until an end-of-sequence token (`<eos>`) is encountered.\n",
    "\n",
    "5. **Output Decoding:**\n",
    "   - Decode the final prediction back into a string format for the generated sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to generate the text from the model\n",
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry is a bit of his father .\n",
      "\n",
      "0.7\n",
      "harry is a report to stop to you . i reckon i could not tell what he was doing . i are sure about you .\n",
      "\n",
      "0.75\n",
      "harry is lucky about it , i wish to be standing over the imperius curse with those wizarding eyes previously , but still as the result of the course there was always\n",
      "\n",
      "0.8\n",
      "harry is lucky about it , i wish to be standing over the imperius curse with those wizarding eyes previously , but still as the result of the course there was always\n",
      "\n",
      "1.0\n",
      "harry is lucky about her , i wish to see ron laughing and though it ought to be fooled he previously , but still as the result of his head there was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the parameters for generation\n",
    "prompt = 'Harry is'\n",
    "max_seq_len = 30\n",
    "seed = SEED\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "# loop through the temperatures\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a nice result! Let's save the model and vocabulary again for later use in the Flask application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model (just in case :D)\n",
    "torch.save(model.state_dict(), './app/lstm_lm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocab as the pickle file for the app\n",
    "import pickle\n",
    "with open('./app/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
